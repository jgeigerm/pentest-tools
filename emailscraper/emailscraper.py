from urllib2 import urlparse, urlopen, HTTPError
from BeautifulSoup import BeautifulSoup as soup
import sys
import argparse
import re
from signal import signal, SIGINT
from time import sleep

#regex for email addresses and multiple slashes
mailreg = re.compile("[\w\d._%+-]+@[\w\d.-]+\.\w+")
slashreg = re.compile("//+|(\./)+")

#verbose variable
verbose = False

#global signal handler, if you hit control-c it will write out what it has collected so far
def sig_handle(sig, frame):
    print("SIGINT... exiting")
    sys.exit(1)


def main():
    #argument parser
    parser = argparse.ArgumentParser()
    parser.add_argument('-t', help="The host to scrape", action='store', dest='host', required=True)
    parser.add_argument('-o', help="Output file for email addresses", action='store', dest='outfile')
    parser.add_argument('-n', help="Number of email addresses to collect", action='store', dest='ncollect', default=20)
    parser.add_argument('-v', help="verbose output", action='store_true', dest='verbose')
    parser.add_argument('-w', help="wait time between requests", action='store', dest='wait')
    args = parser.parse_args()

    #signal setup with globals
    signal(SIGINT, sig_handle)

    if args.ncollect is None:
        args.ncollect = 20
    if args.wait is None:
        args.wait = 0

    #setup output file name
    if not args.outfile is None:
        outfile = args.outfile
    else:
        outfile = "{}.emails.txt".format(urlparse.urlparse(args.host).netloc)
    emails = []

    #check verbose
    if args.verbose:
        global verbose
        verbose = True

    #scrape, if the first scrape fails, the link is invalid
    if (scrape(args.host, int(args.ncollect), [], emails, int(args.wait), True)):
        with open(outfile, "a") as file: #save emails when finished
            for email in emails:
                file.write(email)
                file.write("\n")
    else:
        print("Link read failed: {}".format(args.host))

    return 0

#scrape function
def scrape(host, number, paths, emails, wait, first):
    #setup signals and verbose flag
    signal(SIGINT, sig_handle)
    global verbose

    #check if the number of emails we requested has been met
    if len(emails) >= number:
        return True

    #get host site path
    hosturl = urlparse.urlparse(host)
    if hosturl.path in paths:
        return True
    paths.append(hosturl.path)

    #get page content
    sleep(wait)
    try:
        content = soup(urlopen(host).read())
        if verbose:
            print("Link read: {}".format(host))
    except Exception as e:
        if isinstance(e, KeyboardInterrupt): #write the file and quit if SigInt
            sig_handle(1,2)
        else:
            if verbose:
                print("Link read failed: {}".format(host))
            return False


    #grab all email addresses from the page and then add it to the list
    pageemails = mailreg.findall(content.prettify())
    for pemail in pageemails:
        if not pemail in emails:
            print("Email found: {}".format(pemail))
            emails.append(pemail)

    #scrape for URLs on the site
    links = []
    for x in content('a'): #find all anchors
        try: links.append(x['href'])
        except: pass
    for link in links:
        plink = urlparse.urlparse(link)
        if plink.path is u'': #if there is no path then just skip it
            continue
        if (hosturl.netloc in plink.netloc or plink.netloc is "") and plink.path not in paths:
            if plink.path[0] == '/': #absolute path
                url = slashreg.sub("/", "{}/{}".format(hosturl.netloc, plink.path))
            elif first is True: #first page check
                url = slashreg.sub("/", "{}/{}/{}".format(hosturl.netloc, hosturl.path, plink.path))
                url = "{}://{}".format(hosturl.scheme, url)
                if (scrape(url, number, paths, emails, wait, False)):
                    continue
                else: #we are at a full path, subtract the file name and start from the path
                    url = slashreg.sub("/", "{}/{}/{}".format(hosturl.netloc, "/".join(hosturl.path.split("/")[:-1]), plink.path))
            else: #all other pages, subtract filename and go from the path
                url = slashreg.sub("/", "{}/{}/{}".format(hosturl.netloc, "/".join(hosturl.path.split("/")[:-1]), plink.path))
            url = "{}://{}".format(hosturl.scheme, url)
            scrape(url, number, paths, emails, wait, False) #recursion!

    return True

if __name__ == '__main__':
    sys.exit(main()) #exit when done
